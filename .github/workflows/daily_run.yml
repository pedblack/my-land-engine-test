name: Daily Land Search & Map

on:
  schedule:
    # Runs at 08:00 UTC every day
    - cron: '0 8 * * *'
  workflow_dispatch: # Allows manual trigger from the Actions tab

jobs:
  scrape_and_visualize:
    runs-on: ubuntu-latest
    timeout-minutes: 60 # Safety for deep crawling
    permissions:
      contents: write

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Install Playwright Browsers & System Deps
        run: |
          # This still needs to be run separately after the pip install
          playwright install chromium --with-deps

      - name: Run Atomic Scraper & Gemini AI Analysis
        run: python backbone_crawler.py
        env:
          # Ensure you have added GOOGLE_API_KEY to Settings > Secrets > Actions
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}

      - name: Generate Geospatial Visualization
        run: python visualize_land.py

      - name: Commit and Push Updated Data & Map
        run: |
          git config --global user.name "P4N Bot"
          git config --global user.email "bot@github.com"
          
          # Stage the database and the interactive HTML map
          git add backbone_locations.csv portugal_land_map.html
          
          # Commit only if files actually changed
          git commit -m "Auto-update Land Database & Map [skip ci]" || echo "No changes to commit"
          
          # Use rebase to handle race conditions if the repo was modified during the run
          git pull --rebase origin main
          git push origin main
